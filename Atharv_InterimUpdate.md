# Update

I have primarily focused on the technical aspects of the project so far. So, I first looked into Zillow and BEAâ€™s data usage policy so that we can adhere to ethical data handling and communicated that to Dhwani so she would acquire the data. Then after Dhwani changed the format of the BEA data and collected both datasets, I created the repository structure. After, I began cleaning the datasets, Dhwani and I both came up with the examine_data() function, then we talked through the logic for selecting 100 largest cities. The data cleaning I conducted included adjusting both file formats to CSV in acquisition along with Dhwani, conducting a detailed examination of the data's hierarchy, meta data, and null values, and filtering null values via our forward filling algorithm for Zillow, sizerank adjustments for the Bureaua of labor statistics data, and the logic behind only examining the US's top 100 metros for data ccuracy and completeness reasons along with relevance to the vast majority of Americans - many of whom live in these major cities. After our discussion of the top 100 metro criteria, I implemented the fuzzy matching and Levenstein methods to align the cities and ensure that minor naming errors do not impact our analysis. This is 95% complete at the moment and is simply being tuned further before we head into pure data analysis. Now that both datasets are mostly cleaned, I am working on integrating them into one wholistic dataset, so that we can understand the relationships between house price, income, and overall affordability. Because our BEA data is from 2023-2024, we will be utilizing this integration in order to analyze present-day affordability trends when it comes to income, along with long-term appreciation trends (200-2024) in the Zillow data with regards to house price. 